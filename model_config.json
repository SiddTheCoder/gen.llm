{
  "models": {
    "qwen2.5-7b": {
      "cpu": {
        "url": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/resolve/main/qwen2.5-7b-instruct-q3_k_m.gguf",
        "filename": "qwen2.5-7b-instruct-q3_k_m.gguf",
        "size_gb": 3.81,
        "quantization": "q3_k_m",
        "description": "Single file, good for CPU"
      },
      "gpu": {
        "url": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/resolve/main/qwen2.5-7b-instruct-q6_k.gguf",
        "filename": "qwen2.5-7b-instruct-q6_k.gguf",
        "size_gb": 6.25,
        "quantization": "q6_k",
        "description": "Single file, best quality for GPU"
      }
    }
  },
  "llama_cpp_binaries": {
    "cpu": {
      "url": "https://github.com/ggerganov/llama.cpp/releases/download/b4480/llama-b4480-bin-win-avx2-x64.zip",
      "filename": "llama-b4480-bin-win-avx2-x64.zip",
      "executable": "llama-cli.exe",
      "size_mb": 50,
      "description": "CPU-optimized (AVX2)"
    },
    "gpu": {
      "url": "https://github.com/ggerganov/llama.cpp/releases/download/b4480/llama-b4480-bin-win-cuda-cu12.2.0-x64.zip",
      "filename": "llama-b4480-bin-win-cuda-cu12.2.0-x64.zip",
      "executable": "llama-cli.exe",
      "size_mb": 450,
      "description": "GPU-optimized (CUDA 12.2)"
    }
  },
  "settings": {
    "models_base_dir": "qwen",
    "binaries_base_dir": "tools",
    "auto_detect_device": true,
    "force_cpu": false
  }
}
